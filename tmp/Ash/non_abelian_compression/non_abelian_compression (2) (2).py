# -*- coding: utf-8 -*-
"""Non Abelian Compression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a2-l9teyAdByhR9FHxM3ltC3sYi5BzI9

---

# Part I — Experiment and Formal Setup

**Interaction-Driven Coherence and Compression in a Non-Abelian Ecology**

## 1. Problem statement (precise scope)

We study the following restricted question:

> **Given a population of systems with hidden non-Abelian structure, can interaction alone induce coherent identities, and can those identities be extracted and compressed without loss with respect to the interaction task?**

This is **not** a study of supervised group learning, rotation estimation, or representation optimality. The goal is to isolate a single mechanism: **coherence emerging from consistency constraints under non-commutative composition**.

---

## 2. Hidden generative structure

### 2.1 System space

Let there be ( N ) systems indexed by ( s \in {1,\dots,N} ).

Each system has an associated, fixed but unobserved element
[
R_s \in \mathrm{SO}(3).
]

These elements are drawn independently at initialization and remain constant throughout the experiment. They are never used during training, only for evaluation.

---

### 2.2 Scene space

A *scene* is a finite set of points in ( \mathbb{R}^3 ):
[
S = {x_1,\dots,x_P} \subset \mathbb{R}^3,
\quad S \in \mathbb{R}^{P\times 3}.
]

Scenes are sampled i.i.d. from a zero-mean distribution and normalized so that
[
\operatorname{Var}(S) \approx 1,
]
to avoid trivial scale effects.

---

### 2.3 Observation model

Given a scene ( S ) and system ( s ), the observed point cloud is
[
X_s = S R_s^\top + \varepsilon,
\quad \varepsilon \sim \mathcal{N}(0, \sigma^2 I).
]

All points are treated as **row vectors**, hence the right multiplication by ( R_s^\top ).

There is:

* no access to ( S ),
* no access to ( R_s ),
* no shared reference frame across systems.

The only guaranteed structure is that **two systems observing the same interaction share the same underlying scene**.

---

## 3. Interaction as the sole learning signal

An *interaction* is defined as the tuple
[
(i,j,X_i,X_j),
]
where ( i \neq j ) and both observations arise from the same ( S ).

The learning signal is the **constraint**:
[
X_i \sim X_j \quad \text{(via unknown, non-commuting transforms)}.
]

No additional labels or metadata are available.

---

## 4. Model architecture (formal description)

The model consists of three components:

---

### 4.1 Scene encoder

A permutation-invariant encoder
[
g_\theta : \mathbb{R}^{P \times 3} \to \mathbb{R}^h
]
maps a point cloud to a latent feature vector.

This encoder is shared across all systems and interactions. It is intentionally generic and does not enforce equivariance explicitly.

---

### 4.2 Persistent system state

Each system ( s ) is associated with a persistent state vector
[
e_s \in \mathbb{R}^d.
]

These states:

* are stored across interactions,
* are updated by gradient descent,
* are **not supervised**,
* are **not reset** between interactions.

The state should be interpreted as *interaction-accumulated internal memory*, not as a representation of ( R_s ).

---

### 4.3 Rotation inference head

A shared mapping
[
h_\theta : \mathbb{R}^h \times \mathbb{R}^d \to \mathrm{SO}(3)
]
predicts a rotation matrix using a 6D parameterization projected onto SO(3).

The predicted rotation for system ( s ) in a given interaction is
[
\hat R_s = h_\theta(g_\theta(X_s), e_s).
]

---

## 5. Interaction loss (no privileged frame)

The predicted rotations are used to *recover* a latent scene:

[
\hat S_s = X_s \hat R_s.
]

Because ( X_s = S R_s^\top ), perfect recovery would satisfy
[
\hat R_s = R_s,
\quad \hat S_s = S.
]

However, neither ( R_s ) nor ( S ) is known or enforced.

The interaction loss for a pair ( (i,j) ) is defined as
[
\mathcal{L}_{ij}
= \left| X_i \hat R_i - X_j \hat R_j \right|_F^2.
]

This loss has three important properties:

1. **Gauge-free**: no absolute reference frame exists.
2. **Pairwise only**: no global scene variable is introduced.
3. **Non-commutative**: consistency depends on composition order through ( \hat R_i, \hat R_j \in \mathrm{SO}(3) ).

---

## 6. Training protocol

### 6.1 Phase I — Pretraining (interaction grammar)

A subset ( \mathcal{S}_{\text{train}} \subset {1,\dots,N} ) is selected.

We jointly optimize:
[
\theta \quad \text{and} \quad {e_s : s \in \mathcal{S}*{\text{train}}}
]
by minimizing expected interaction loss over random pairs in ( \mathcal{S}*{\text{train}} ).

Interpretation:

* The model learns *how* rotations should behave to satisfy interaction consistency.
* This phase does **not** produce transferable identities for unseen systems.

---

### 6.2 Phase II — Ecology bootstrap (state-only learning)

For a disjoint set ( \mathcal{S}_{\text{test}} ):

* Model parameters ( \theta ) are frozen.
* Only state vectors ( e_s ) for ( s \in \mathcal{S}_{\text{test}} ) are updated.
* Learning proceeds solely via interaction losses among test systems.

This phase isolates the effect of **interaction-driven state adaptation**.

---

## 7. Evaluation observables

We evaluate three quantities.

---

### 7.1 Scene consistency

[
\mathbb{E}_{i,j,S},
\left| X_i \hat R_i - X_j \hat R_j \right|_F^2.
]

This measures whether systems can agree on a recovered scene under fresh interactions.

---

### 7.2 Relative rotation geometry

Let
[
d_{\mathrm{SO}(3)}(A,B)
= \arccos!\left(\frac{\operatorname{tr}(A^\top B)-1}{2}\right)
]
denote the geodesic distance on SO(3).

We measure correlation between
[
d_{\mathrm{SO}(3)}(\hat R_i,\hat R_j)
\quad \text{and} \quad
d_{\mathrm{SO}(3)}(R_i,R_j).
]

This tests whether the inferred transformations preserve true group geometry.

---

### 7.3 State-space geometry

We also measure correlation between
[
| e_i - e_j |*2
\quad \text{and} \quad
d*{\mathrm{SO}(3)}(R_i,R_j).
]

This probes how much geometric information is stored in persistent state.

---

## 8. Empirical results (summary)

Empirically we observe:

1. **Before ecology**
   Unseen systems show:

   * high scene inconsistency,
   * no geometric alignment,
   * unstructured state space.

2. **After ecology**
   Without changing model parameters:

   * scene consistency matches trained systems,
   * inferred rotations align with true SO(3) geometry,
   * state vectors partially reflect group structure.

These effects arise purely from interaction-driven state updates.

---

## 9. Meta-ecology: extracting canonical identity

After ecology stabilizes, we construct for each system ( s ) a *canonical transform*:

[
R_s^\ast
========

\Pi_{\mathrm{SO}(3)}
\left(
\frac{1}{K} \sum_{k=1}^K
\hat R_s^{(k)}
\right),
]
where:

* ( \hat R_s^{(k)} ) are predictions from independent scenes,
* ( \Pi_{\mathrm{SO}(3)} ) denotes projection to the nearest rotation matrix.

This defines a single, stable gauge choice per system.

---

## 10. Compression test (formal statement)

We define a compressed ecology consisting only of
[
{ R_s^\ast : s \in \mathcal{S}_{\text{test}} }.
]

The compressed recovered scene is
[
\tilde S_s = X_s R_s^\ast.
]

We compare:

* Full ecology performance:
  [
  \mathbb{E},| X_i \hat R_i - X_j \hat R_j |^2
  ]
* Compressed ecology performance:
  [
  \mathbb{E},| X_i R_i^\ast - X_j R_j^\ast |^2.
  ]

Empirically these quantities are indistinguishable within noise.

---

## 11. What Part I establishes (only)

From this experiment alone, we can conclude:

1. Interaction-only learning can induce coherent identities in a non-Abelian setting.
2. These identities can be made explicit as canonical transformations.
3. Once explicit, the ecology can be compressed by orders of magnitude without loss **for the interaction task**.

No broader claims are made at this stage.

---

---

# Part II — Interpretation and Theory

**Why Interaction Produces Coherence and Why Compression Was Possible**

## 12. What must be explained

Part I establishes three empirical facts:

1. Interaction-only learning induces coherent identities among systems with hidden non-Abelian structure.
2. These identities can be made explicit as canonical transformations ( R_s^\ast ).
3. Replacing the full ecology with ( {R_s^\ast} ) preserves all measured interaction observables.

Part II asks **why** this is possible, and **what kind of losslessness** is involved.

---

## 13. Interaction induces a relational, not representational, structure

The experiment never optimizes toward ground-truth rotations. Instead, it enforces **pairwise consistency**:

[
X_i \hat R_i ;\approx; X_j \hat R_j.
]

This constraint is **relational**: it compares outcomes of two systems acting on the *same* scene. No absolute quantity is ever observed.

As a result, the learned structure is not a map
[
X \mapsto R,
]
but a rule ensuring that *different* ( X ) become compatible under learned transformations.

This distinction matters: the ecology is defined by **relations between systems**, not by system-wise representations.

---

## 14. The implicit algebraic object: a groupoid

The interaction structure induced by the ecology is best described as a **groupoid**, not a group.

* **Objects**: systems ( s )
* **Morphisms**: effective transports between systems
  [
  T_{ij} := \hat R_j^{-1} \hat R_i
  ]
* **Composition**: induced by chaining interactions
* **Identities**: ( T_{ii} = I )

Because rotations do not commute, this groupoid is non-Abelian in general.

The loss enforces **local consistency** of morphisms:
[
T_{ij} T_{jk} \approx T_{ik},
]
but never forces a global coordinate choice.

---

## 15. Persistence and loop closure

The persistent state vectors ( e_s ) serve one role only:

> to accumulate interaction history so that repeated loop constraints can be satisfied.

Consider a loop ( i \to j \to k \to i ). Consistency requires:
[
T_{ij} T_{jk} T_{ki} \approx I.
]

Because composition is non-commutative, this constraint cannot be satisfied by simple averaging. Instead, the system must adjust internal state until **loop holonomy stabilizes**.

In practice, this means:
[
e_s ;\text{stops changing} \quad \Longleftrightarrow \quad \text{loop errors vanish}.
]

Thus, *identity* emerges as a **fixed point of interaction dynamics**.

---

## 16. Sameness as a fixed point, not a label

Crucially, the experiment never labels systems as “the same” or “different.”

Instead, sameness is defined operationally:

> A system is stable if further interactions do not change its recovered-scene behavior.

Formally, for system ( s ), stability means:
[
\mathbb{E}_{S,i},
\bigl|
X_s \hat R_s - X_i \hat R_i
\bigr|^2
;\text{is minimized and stationary}.
]

This definition is stronger than explicit labeling: sameness is *earned* by satisfying all relational constraints.

---

## 17. Why non-Abelian structure does not prevent compression

Non-Abelian groups are often considered incompressible because:

* they lack global coordinates,
* composition is path-dependent,
* curvature prevents flattening.

This experiment does **not** contradict those facts.

What is compressed is **not** the group ( \mathrm{SO}(3) ) itself, but a **resolved orbit**:

[
\text{non-Abelian ecology}
;\xrightarrow{;\text{loop closure};}
\text{one stable gauge choice per system}.
]

Once loop constraints are satisfied, remaining gauge freedom collapses to a constant.

---

## 18. Compression as a quotient

The compression performed is formally a **quotient** operation.

Let ( \mathcal{E} ) denote the full ecology (model + states). Define an equivalence relation:
[
\mathcal{E}_1 \sim \mathcal{E}_2
\quad \text{iff} \quad
\forall (i,j):;
| X_i \hat R_i^{(1)} - X_j \hat R_j^{(1)} |
===========================================

| X_i \hat R_i^{(2)} - X_j \hat R_j^{(2)} |.
]

The compressed object ( {R_s^\ast} ) represents an equivalence class under this relation.

Because all measured observables depend only on this equivalence class, compression is **lossless relative to the interaction task**.

---

## 19. Why the compression works *after* ecology, not before

Before ecology:

* states ( e_s ) are arbitrary,
* no canonical choice exists,
* quotient is ill-defined.

After ecology:

* loop holonomy vanishes,
* identities stabilize,
* canonical representatives exist.

Thus:
[
\text{coherence} ;\Rightarrow; \text{quotientability}.
]

Compression is therefore a **consequence**, not a cause, of interaction-driven coherence.

---

## 20. Interpreting “losslessness” precisely

The experiment supports the following claim:

> The compressed object preserves all observables expressible as recovered-scene consistency under interaction.

It does **not** claim:

* universality across tasks,
* optimality of the representation,
* invariance under arbitrary downstream objectives.

Losslessness is **task-relative** and explicitly scoped.

---

## 21. Why this does not trivialize non-Abelian complexity

The non-Abelian structure is not removed; it is **used**.

* Non-commutativity forces loop constraints.
* Loop constraints force stabilization.
* Stabilization enables a quotient.

In Abelian settings, these steps collapse trivially and do not produce informative fixed points. This explains why analogous constructions often fail in commutative problems.

---

## 22. Limitations and boundaries

This interpretation relies on:

* persistent state,
* repeated interactions,
* closed-loop exposure,
* a well-behaved transformation group.

We do not claim that:

* all non-Abelian systems admit such quotients,
* compression is always finite-dimensional,
* or that neural networks are necessary.

The result is conditional and structural.

---

## 23. Summary of Part II

From a theoretical perspective, the experiment shows:

1. Interaction induces a groupoid-like relational structure.
2. Persistent state enables loop closure in non-commutative settings.
3. Identity emerges as a fixed point of interaction dynamics.
4. Once identities stabilize, a quotient exists.
5. That quotient can be dramatically smaller while preserving interaction behavior.

This explains **why** lossless compression was possible in Part I, without invoking representation learning or explicit supervision.

---
"""

# ============================================================
# LEARNING ECOLOGY on a NON-ABELIAN GROUP (SO(3))
# + Meta-ecology "SAMENESS" extraction
# + Lossless-ish COMPRESSION report
#
# What you asked for:
# 1) Run the usual 2-phase ecology (pretrain on train systems, then bootstrap on unseen test systems via state-only updates)
# 2) Find "SAMENESS" in the meta-ecology:
#    - sameness := existence of a shared canonical latent scene space where recovered scenes agree
#    - operationally: Xi @ R_est[i] and Xj @ R_est[j] match for co-observed scenes
# 3) Compress:
#    - export one rotation per system (quaternion) as the compressed ecology object
#    - optionally export only TEST systems if you want
# 4) Print:
#    - memory (bytes) of original (model + embeddings)
#    - memory of compressed (quats)
#    - compression ratio
#    - “losslessness” = how well compressed object preserves the ecology function:
#         pair-MSE, and corr(dist(R), true), compared to full model+state
#
# Notes:
# - This compression is lossless in the *ecology-function* sense if the compressed quats
#   reproduce the same recovered-scene agreement statistics (pair MSE) up to tiny epsilon.
# - It is not claiming a coordinate-chart is unique; this is a gauge choice.
# ============================================================

import math, time
import torch
import torch.nn as nn
import torch.nn.functional as F

# ----------------------------
# device / seeds
# ----------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(0)
print("device:", device)

# ----------------------------
# SO(3) utilities
# ----------------------------
def rand_quat(n, device):
    # uniform random quaternion (n,4) as (x,y,z,w)
    u1 = torch.rand(n, device=device)
    u2 = torch.rand(n, device=device)
    u3 = torch.rand(n, device=device)
    q = torch.zeros(n, 4, device=device)
    q[:, 0] = torch.sqrt(1 - u1) * torch.sin(2 * math.pi * u2)
    q[:, 1] = torch.sqrt(1 - u1) * torch.cos(2 * math.pi * u2)
    q[:, 2] = torch.sqrt(u1) * torch.sin(2 * math.pi * u3)
    q[:, 3] = torch.sqrt(u1) * torch.cos(2 * math.pi * u3)
    q = q / (q.norm(dim=-1, keepdim=True) + 1e-9)
    return q

def quat_to_R(q):
    # q: (...,4) as (x,y,z,w)
    x, y, z, w = q.unbind(-1)
    xx, yy, zz = x*x, y*y, z*z
    xy, xz, yz = x*y, x*z, y*z
    wx, wy, wz = w*x, w*y, w*z
    R = torch.stack([
        1 - 2*(yy+zz), 2*(xy-wz),     2*(xz+wy),
        2*(xy+wz),     1 - 2*(xx+zz), 2*(yz-wx),
        2*(xz-wy),     2*(yz+wx),     1 - 2*(xx+yy)
    ], dim=-1).reshape(q.shape[:-1] + (3,3))
    return R

def R_to_quat(R):
    # robust-ish conversion R -> (x,y,z,w)
    # (not differentiable; used only for export)
    # Based on standard trace method; stable enough for evaluation/export.
    tr = R[...,0,0] + R[...,1,1] + R[...,2,2]
    qw = torch.sqrt(torch.clamp(1.0 + tr, min=1e-9)) * 0.5
    qx = (R[...,2,1] - R[...,1,2]) / (4*qw + 1e-9)
    qy = (R[...,0,2] - R[...,2,0]) / (4*qw + 1e-9)
    qz = (R[...,1,0] - R[...,0,1]) / (4*qw + 1e-9)
    q = torch.stack([qx,qy,qz,qw], dim=-1)
    q = q / (q.norm(dim=-1, keepdim=True) + 1e-9)
    return q

def ortho6d_to_R(x6):
    # Zhou et al. 6D rep -> SO(3)
    a1 = x6[..., 0:3]
    a2 = x6[..., 3:6]
    b1 = F.normalize(a1, dim=-1)
    b2 = F.normalize(a2 - (b1 * a2).sum(dim=-1, keepdim=True) * b1, dim=-1)
    b3 = torch.cross(b1, b2, dim=-1)
    R = torch.stack([b1, b2, b3], dim=-1) # columns
    return R

def so3_geodesic(Ra, Rb):
    M = Ra.transpose(-1, -2) @ Rb
    tr = M[..., 0, 0] + M[..., 1, 1] + M[..., 2, 2]
    c = (tr - 1) * 0.5
    c = c.clamp(-1.0, 1.0)
    return torch.acos(c)

def project_to_SO3(M):
    # nearest rotation via SVD: R = U V^T with det +1
    U, S, Vh = torch.linalg.svd(M)
    R = U @ Vh
    # fix reflection if det negative
    det = torch.linalg.det(R)
    if det.ndim == 0:
        if det < 0:
            U[:, -1] *= -1
            R = U @ Vh
    else:
        bad = det < 0
        if bad.any():
            U2 = U.clone()
            U2[bad, :, -1] *= -1
            R = U2 @ Vh
    return R

# ----------------------------
# Synthetic "systems"
# ----------------------------
def sample_scene(B, P, device):
    S = torch.randn(B, P, 3, device=device)
    S = S / (S.std(dim=(1,2), keepdim=True) + 1e-6)
    return S

def observe(S, R, noise=0.02):
    # row vectors: X = S R^T
    X = S @ R.transpose(-1, -2)
    if noise > 0:
        X = X + noise * torch.randn_like(X)
    return X

# ----------------------------
# Model: encoder + head + per-system state
# ----------------------------
class PointEncoder(nn.Module):
    def __init__(self, h=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(3, h), nn.ReLU(),
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, h),
        )
        self.post = nn.Sequential(
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, h),
        )
    def forward(self, X):
        z = self.mlp(X)          # (B,P,h)
        z = z.mean(dim=1)        # (B,h)
        z = self.post(z)
        return z

class RotHead(nn.Module):
    def __init__(self, h=128, e_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(h + e_dim, h), nn.ReLU(),
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, 6),
        )
    def forward(self, feat, e):
        x6 = self.net(torch.cat([feat, e], dim=-1))
        return ortho6d_to_R(x6)

class EcologySO3(nn.Module):
    def __init__(self, h=128, e_dim=32):
        super().__init__()
        self.enc = PointEncoder(h=h)
        self.head = RotHead(h=h, e_dim=e_dim)

    def infer_R(self, X, e):
        feat = self.enc(X)
        return self.head(feat, e)

# ----------------------------
# Setup: population splits
# ----------------------------
N_total = 220
N_train = 80
N_test  = 80
N_hold  = N_total - N_train - N_test

e_dim = 32
H = 128

# hidden true rotations (eval only)
q_true = rand_quat(N_total, device=device)
R_true = quat_to_R(q_true)  # (N,3,3)

train_ids = torch.arange(0, N_train, device=device)
test_ids  = torch.arange(N_train, N_train + N_test, device=device)
hold_ids  = torch.arange(N_train + N_test, N_total, device=device)

# persistent per-system state
E = nn.Embedding(N_total, e_dim).to(device)
nn.init.normal_(E.weight, std=0.2)

# model
model = EcologySO3(h=H, e_dim=e_dim).to(device)

# ----------------------------
# Interaction sampler
# ----------------------------
@torch.no_grad()
def sample_pairs(ids, B):
    idx = ids[torch.randint(0, len(ids), (B,), device=device)]
    jdx = ids[torch.randint(0, len(ids), (B,), device=device)]
    mask = idx == jdx
    if mask.any():
        jdx[mask] = ids[torch.randint(0, len(ids), (int(mask.sum().item()),), device=device)]
    return idx, jdx

def interaction_batch(idx, jdx, P=48, noise=0.02):
    B = idx.shape[0]
    S  = sample_scene(B, P, device=device)
    Ri = R_true[idx]
    Rj = R_true[jdx]
    Xi = observe(S, Ri, noise=noise)
    Xj = observe(S, Rj, noise=noise)
    return Xi, Xj

# ----------------------------
# Loss: no privileged frame
# ----------------------------
def loss_interaction(Xi, Xj, ei, ej):
    Ri_hat = model.infer_R(Xi, ei)
    Rj_hat = model.infer_R(Xj, ej)
    # recovered scene for row-vectors: X @ R_hat  (since X = S @ R^T)
    Si_hat = Xi @ Ri_hat
    Sj_hat = Xj @ Rj_hat
    loss_pair = F.mse_loss(Si_hat, Sj_hat)
    return loss_pair, Ri_hat, Rj_hat

# ----------------------------
# Phase 1: Pretrain
# ----------------------------
def pretrain(steps=6000, B=256, lr=2e-3, P=48, noise=0.02, print_every=500):
    opt = torch.optim.Adam(list(model.parameters()) + list(E.parameters()), lr=lr)
    ema = None
    t0 = time.time()
    model.train()

    for step in range(1, steps+1):
        idx, jdx = sample_pairs(train_ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        ei, ej = E(idx), E(jdx)
        loss_pair, _, _ = loss_interaction(Xi, Xj, ei, ej)

        opt.zero_grad(set_to_none=True)
        loss_pair.backward()
        opt.step()

        l = float(loss_pair.detach())
        ema = l if ema is None else 0.98*ema + 0.02*l
        if step % print_every == 0 or step == 1:
            print(f"pretrain step {step:5d} | loss={l:.6f} | ema={ema:.6f}")

    print(f"pretrain time: {time.time()-t0:.1f}s")

# ----------------------------
# Phase 2: Ecology bootstrap (state-only updates on TEST)
# ----------------------------
def bootstrap_ecology(steps=8000, B=256, lr_state=1e-2, P=48, noise=0.02, print_every=500):
    for p in model.parameters():
        p.requires_grad_(False)
    model.eval()

    opt = torch.optim.Adam([E.weight], lr=lr_state)
    ema = None
    t0 = time.time()

    for step in range(1, steps+1):
        idx, jdx = sample_pairs(test_ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        ei, ej = E(idx), E(jdx)

        loss_pair, _, _ = loss_interaction(Xi, Xj, ei, ej)
        opt.zero_grad(set_to_none=True)
        loss_pair.backward()

        # mask grads: only update TEST rows
        with torch.no_grad():
            g = E.weight.grad
            if g is not None:
                mask = torch.zeros(N_total, device=device, dtype=torch.bool)
                mask[test_ids] = True
                g[~mask] = 0.0

        opt.step()

        l = float(loss_pair.detach())
        ema = l if ema is None else 0.98*ema + 0.02*l
        if step % print_every == 0 or step == 1:
            print(f"eco step {step:5d} | loss={l:.6f} | ema={ema:.6f}")

    print(f"ecology time: {time.time()-t0:.1f}s")

# ----------------------------
# Evaluation (full ecology object = model + E)
# ----------------------------
@torch.no_grad()
def eval_pair_mse_full(ids, trials=200, B=256, P=48, noise=0.02):
    model.eval()
    mse = 0.0
    for _ in range(trials):
        idx, jdx = sample_pairs(ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        ei, ej = E(idx), E(jdx)
        loss_pair, _, _ = loss_interaction(Xi, Xj, ei, ej)
        mse += float(loss_pair)
    return mse / trials

@torch.no_grad()
def eval_corr_Rhat_true_full(ids, B=512, P=48, noise=0.02):
    model.eval()
    n = len(ids)
    idx = ids[torch.randint(0, n, (B,), device=device)]
    jdx = ids[torch.randint(0, n, (B,), device=device)]
    m = idx == jdx
    if m.any():
        jdx[m] = ids[torch.randint(0, n, (int(m.sum().item()),), device=device)]
    Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
    ei, ej = E(idx), E(jdx)
    Ri_hat = model.infer_R(Xi, ei)
    Rj_hat = model.infer_R(Xj, ej)
    d_hat  = so3_geodesic(Ri_hat, Rj_hat)
    d_true = so3_geodesic(R_true[idx], R_true[jdx])
    d_hat  = d_hat - d_hat.mean()
    d_true = d_true - d_true.mean()
    corr = (d_hat * d_true).mean() / (d_hat.std() * d_true.std() + 1e-9)
    return float(corr)

@torch.no_grad()
def eval_corr_E_true(ids, samples=6000):
    n = len(ids)
    a = ids[torch.randint(0, n, (samples,), device=device)]
    b = ids[torch.randint(0, n, (samples,), device=device)]
    m = a == b
    if m.any():
        b[m] = ids[torch.randint(0, n, (int(m.sum().item()),), device=device)]
    ea, eb = E(a), E(b)
    de = (ea - eb).norm(dim=-1)
    dR = so3_geodesic(R_true[a], R_true[b])
    de = de - de.mean()
    dR = dR - dR.mean()
    corr = (de * dR).mean() / (de.std() * dR.std() + 1e-9)
    return float(corr)

# ----------------------------
# META-ECOLOGY: extract SAMENESS + compressed object
#  - "Sameness" operationally = recovered scenes agree in a shared canonical space.
#  - We build a canonical rotation estimate per system by averaging inferred rotations
#    across many fresh scenes, then projecting mean to SO(3).
#
# Compressed object: q_est[s] (4 floats per system) or store only test_ids.
# Then we evaluate ecology purely via Xi @ R_est[i], without running the NN at all.
# ----------------------------
@torch.no_grad()
def estimate_R_per_system(ids, K=64, P=48, noise=0.02, B_chunk=128):
    """
    For each system s:
      sample K random scenes,
      observe X = S @ R_true[s]^T,
      infer R_hat = model.infer_R(X, E(s)),
      average R_hat, project to SO(3) => R_est[s]
    """
    model.eval()
    ids = ids.clone()
    R_est = torch.empty(len(ids), 3, 3, device=device)

    # do in chunks for speed
    for start in range(0, len(ids), B_chunk):
        chunk = ids[start:start+B_chunk]
        B = len(chunk)

        # accumulate mean rotation matrix
        M = torch.zeros(B, 3, 3, device=device)
        e = E(chunk)

        for _ in range(K):
            S = sample_scene(B, P, device=device)
            X = observe(S, R_true[chunk], noise=noise)
            R_hat = model.infer_R(X, e)
            M += R_hat

        M = M / float(K)
        R_est[start:start+B_chunk] = project_to_SO3(M)

    return R_est

@torch.no_grad()
def eval_pair_mse_compressed(ids, R_est, trials=200, B=256, P=48, noise=0.02):
    """
    Compressed ecology evaluation:
      recovered scene via Xi @ R_est[i] (no NN, no E)
    ids: tensor of system ids
    R_est: (len(ids),3,3) aligned to ids order
    """
    # map system-id -> row index in R_est
    # (use a dense lookup table for speed)
    lut = torch.full((N_total,), -1, device=device, dtype=torch.long)
    lut[ids] = torch.arange(len(ids), device=device)

    mse = 0.0
    for _ in range(trials):
        idx, jdx = sample_pairs(ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)

        Ri = R_est[lut[idx]]
        Rj = R_est[lut[jdx]]

        Si = Xi @ Ri
        Sj = Xj @ Rj
        mse += float(F.mse_loss(Si, Sj))
    return mse / trials

@torch.no_grad()
def eval_corr_Rest_true(ids, R_est, samples=6000):
    """
    Compare distances in compressed rotations vs true distances.
    """
    n = len(ids)
    a = ids[torch.randint(0, n, (samples,), device=device)]
    b = ids[torch.randint(0, n, (samples,), device=device)]
    m = a == b
    if m.any():
        b[m] = ids[torch.randint(0, n, (int(m.sum().item()),), device=device)]

    # LUT id->index
    lut = torch.full((N_total,), -1, device=device, dtype=torch.long)
    lut[ids] = torch.arange(n, device=device)

    Ra = R_est[lut[a]]
    Rb = R_est[lut[b]]
    d_est  = so3_geodesic(Ra, Rb)
    d_true = so3_geodesic(R_true[a], R_true[b])

    d_est  = d_est - d_est.mean()
    d_true = d_true - d_true.mean()
    corr = (d_est * d_true).mean() / (d_est.std() * d_true.std() + 1e-9)
    return float(corr)

# ----------------------------
# Memory / compression accounting
# ----------------------------
def bytes_of_params(module):
    n = 0
    for p in module.parameters():
        n += p.numel() * p.element_size()
    return n

def bytes_of_embedding_rows(emb, ids):
    # float32 rows
    w = emb.weight
    return len(ids) * w.shape[1] * w.element_size()

def bytes_of_quats(n_systems, dtype=torch.float32):
    return n_systems * 4 * torch.tensor([], dtype=dtype).element_size()

# ----------------------------
# RUN (as per usual)
# ----------------------------
print("\n=== PHASE 1: PRETRAIN (train systems) ===")
pretrain(steps=6000, B=256, lr=2e-3, P=48, noise=0.02, print_every=500)

print("\n=== EVAL BEFORE ECOLOGY (test systems) ===")
mse_test_before = eval_pair_mse_full(test_ids, trials=200, B=256, P=48, noise=0.02)
corr_E_before   = eval_corr_E_true(test_ids, samples=6000)
corr_R_before   = eval_corr_Rhat_true_full(test_ids, B=512, P=48, noise=0.02)
print(f"pair MSE test->test (before ecology): {mse_test_before:.6f}")
print(f"corr(dist(E), true SO(3) dist) test (before): {corr_E_before:.4f}")
print(f"corr(dist(R_hat), true dist) test (before): {corr_R_before:.4f}")

print("\n=== PHASE 2: ECOLOGY BOOTSTRAP (test systems; state-only updates) ===")
bootstrap_ecology(steps=8000, B=256, lr_state=1e-2, P=48, noise=0.02, print_every=500)

print("\n=== EVAL AFTER ECOLOGY (test systems) ===")
mse_test_after = eval_pair_mse_full(test_ids, trials=200, B=256, P=48, noise=0.02)
corr_E_after   = eval_corr_E_true(test_ids, samples=6000)
corr_R_after   = eval_corr_Rhat_true_full(test_ids, B=512, P=48, noise=0.02)
print(f"pair MSE test->test (after ecology): {mse_test_after:.6f}")
print(f"corr(dist(E), true SO(3) dist) test (after): {corr_E_after:.4f}")
print(f"corr(dist(R_hat), true dist) test (after): {corr_R_after:.4f}")

# ----------------------------
# META-ECOLOGY: SAMENESS + COMPRESSION
# ----------------------------
print("\n=== META-ECOLOGY: extract SAMENESS as a canonical scene space ===")
print("Estimating one rotation per TEST system (this is the compressed ecology object)...")

R_est_test = estimate_R_per_system(test_ids, K=64, P=48, noise=0.02, B_chunk=128)
q_est_test = R_to_quat(R_est_test)

# "sameness" diagnostic = how well recovered scenes agree under the compressed object
mse_test_comp = eval_pair_mse_compressed(test_ids, R_est_test, trials=200, B=256, P=48, noise=0.02)
corr_Rest     = eval_corr_Rest_true(test_ids, R_est_test, samples=6000)

print(f"compressed pair MSE test->test (Xi@R_est): {mse_test_comp:.6f}")
print(f"corr(dist(R_est), true dist) test:         {corr_Rest:.4f}")

# define “losslessness” in ecology-function sense:
# how close the compressed object matches the full ecology performance
eps = 1e-12
lossless_mse_ratio = (mse_test_comp + eps) / (mse_test_after + eps)
print(f"losslessness (MSE ratio comp/full):         {lossless_mse_ratio:.4f}   (1.0 = perfect)")

# ----------------------------
# Compression accounting
# ----------------------------
orig_model_bytes = bytes_of_params(model)
orig_E_test_bytes = bytes_of_embedding_rows(E, test_ids)
orig_total_bytes = orig_model_bytes + orig_E_test_bytes

comp_bytes = bytes_of_quats(len(test_ids), dtype=torch.float32)

ratio = orig_total_bytes / max(comp_bytes, 1)

def fmt_bytes(b):
    for unit in ["B","KB","MB","GB"]:
        if b < 1024:
            return f"{b:.1f}{unit}"
        b /= 1024
    return f"{b:.1f}TB"

print("\n=== COMPRESSION REPORT (for TEST ecology object) ===")
print(f"Original (model params):               {fmt_bytes(orig_model_bytes)}")
print(f"Original (TEST states E[test]):        {fmt_bytes(orig_E_test_bytes)}")
print(f"Original total (model + E[test]):      {fmt_bytes(orig_total_bytes)}")
print(f"Compressed (quats for TEST systems):   {fmt_bytes(comp_bytes)}")
print(f"Compression ratio (orig / compressed): {ratio:.1f}x")

# optional: export compressed quats
# torch.save({"test_ids": test_ids.detach().cpu(),
#             "q_est_test": q_est_test.detach().cpu()},
#            "compressed_ecology_test_quats.pt")

print("\nDone.")

!pip -q install trimesh

import os, tarfile, numpy as np
import trimesh

# Official Stanford repo tarball (research use; see repo note)
# bunny.tar.gz contains bun_zipper.ply
!wget -q -O bunny.tar.gz http://graphics.stanford.edu/pub/3Dscanrep/bunny.tar.gz

# extract
with tarfile.open("bunny.tar.gz", "r:gz") as tf:
    tf.extractall("bunny_data")

# find bun_zipper.ply
ply_path = None
for root, _, files in os.walk("bunny_data"):
    for f in files:
        if f.lower() == "bun_zipper.ply":
            ply_path = os.path.join(root, f)
            break
    if ply_path: break

print("PLY:", ply_path)

mesh = trimesh.load(ply_path, process=False)
V = np.asarray(mesh.vertices, dtype=np.float32)

# normalize scale (nice for training)
V = V - V.mean(axis=0, keepdims=True)
V = V / (V.std(axis=0, keepdims=True) + 1e-6)

print("vertices:", V.shape, "min/max:", V.min(), V.max())

# ============================================================
# NON-ABELIAN ECOLOGY on SE(3) (camera network) + META-COMPRESSION
# ------------------------------------------------------------
# Core idea:
#  - Pretrain encoder+head on TRAIN cameras using ONLY pairwise scene-consistency.
#  - Freeze model.
#  - ECOLOGY: update ONLY per-camera state embeddings E[test] on TEST cameras from exposure.
#  - META-ECOLOGY compression: extract one canonical (R_est, t_est) per TEST camera by
#    averaging predictions across fresh scenes (then project R_est to SO(3)).
#
# Metrics:
#  - pair MSE on fresh interactions (LIVE)
#  - pose distance correlations (hat vs true) for rotation + translation (eval only)
#  - loop closure *error* (to identity) and relative motion *scale* (sanity)
#  - compressed pair MSE on fresh interactions using only (R_est,t_est)
#  - matched task MSE live vs compressed, losslessness ratio comp/live
#  - reproduce-model error: how close (R_est,t_est) are to model’s instantaneous outputs
#  - compression report: bytes + ratios; float16 estimate
#
# IMPORTANT:
#  - This code assumes you already have a mesh vertex array V (numpy, shape [Nv,3])
#    loaded in a previous cell (e.g., Stanford bunny vertices).
# ============================================================

import math, time
import torch
import torch.nn as nn
import torch.nn.functional as F

# ----------------------------
# device / seeds
# ----------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(0)
print("device:", device)

# ----------------------------
# SO(3) utilities
# ----------------------------
def rand_quat(n, device):
    u1 = torch.rand(n, device=device)
    u2 = torch.rand(n, device=device)
    u3 = torch.rand(n, device=device)
    q = torch.zeros(n, 4, device=device)
    q[:, 0] = torch.sqrt(1 - u1) * torch.sin(2 * math.pi * u2)
    q[:, 1] = torch.sqrt(1 - u1) * torch.cos(2 * math.pi * u2)
    q[:, 2] = torch.sqrt(u1) * torch.sin(2 * math.pi * u3)
    q[:, 3] = torch.sqrt(u1) * torch.cos(2 * math.pi * u3)
    q = q / (q.norm(dim=-1, keepdim=True) + 1e-9)
    return q

def quat_to_R(q):
    # q: (...,4) as (x,y,z,w)
    x, y, z, w = q.unbind(-1)
    xx, yy, zz = x*x, y*y, z*z
    xy, xz, yz = x*y, x*z, y*z
    wx, wy, wz = w*x, w*y, w*z
    R = torch.stack([
        1 - 2*(yy+zz), 2*(xy-wz), 2*(xz+wy),
        2*(xy+wz), 1 - 2*(xx+zz), 2*(yz-wx),
        2*(xz-wy), 2*(yz+wx), 1 - 2*(xx+yy)
    ], dim=-1).reshape(q.shape[:-1] + (3,3))
    return R

def ortho6d_to_R(x6):
    # Zhou et al. 6D representation -> SO(3)
    a1 = x6[..., 0:3]
    a2 = x6[..., 3:6]
    b1 = F.normalize(a1, dim=-1)
    b2 = F.normalize(a2 - (b1 * a2).sum(dim=-1, keepdim=True) * b1, dim=-1)
    b3 = torch.cross(b1, b2, dim=-1)
    return torch.stack([b1, b2, b3], dim=-1)  # columns

def so3_geodesic(Ra, Rb):
    # angle = arccos((tr(Ra^T Rb)-1)/2)
    M = Ra.transpose(-1, -2) @ Rb
    tr = M[..., 0, 0] + M[..., 1, 1] + M[..., 2, 2]
    c = ((tr - 1) * 0.5).clamp(-1.0, 1.0)
    return torch.acos(c)

def project_to_SO3(R):
    # nearest rotation via SVD (batch-safe)
    U, _, Vt = torch.linalg.svd(R)
    M = U @ Vt
    det = torch.det(M)
    bad = det < 0
    if bad.any():
        U = U.clone()
        U[bad, :, -1] *= -1
        M = U @ Vt
    return M

# ----------------------------
# Load bunny points into torch
# ----------------------------
# Expect: V is a numpy array of vertices (Nv,3) from previous cell.
try:
    V_t = torch.from_numpy(V).to(device)
except NameError as e:
    raise NameError(
        "Missing V (numpy vertices). Load a mesh in a previous cell, e.g. bunny vertices as V."
    ) from e

@torch.no_grad()
def sample_scene_from_mesh(B, P):
    idx = torch.randint(0, V_t.shape[0], (B, P), device=device)
    S = V_t[idx]  # (B,P,3)
    S = S - S.mean(dim=1, keepdim=True)
    S = S / (S.std(dim=(1,2), keepdim=True) + 1e-6)
    return S

# ----------------------------
# SE(3) observation model (ROW vectors)
# World points S -> camera points X
#   X = (S - t) @ R^T + noise
# Recover:
#   S_hat = X @ R + t
# ----------------------------
def observe_se3(S, R, t, noise=0.01):
    X = (S - t[:, None, :]) @ R.transpose(-1, -2)
    if noise > 0:
        X = X + noise * torch.randn_like(X)
    return X

# ============================================================
# LOOP CLOSURE (SE(3), ROW vectors) + SCALE REPORTING
# ------------------------------------------------------------
# Absolute pose convention:
#   X = (S - t) @ R^T
# Derived camera-to-camera map (j -> i):
#   x_i = (x_j - t_ij) @ R_ij^T
# with:
#   R_ij = R_i @ R_j^T
#   t_ij = (t_i - t_j) @ R_j^T     (expressed in camera-j coords)
#
# Composition (a <- b) ∘ (b <- c) = (a <- c):
#   R_ac = R_ab @ R_bc
#   t_ac = t_bc + (t_ab @ R_bc)
#
# Loop closure error over i<-j<-k<-i should be identity:
#   R_loop ~ I, t_loop ~ 0
# ============================================================

def cam_rel(Ri, ti, Rj, tj):
    R_ij = Ri @ Rj.transpose(-1, -2)
    t_ij = (ti - tj) @ Rj.transpose(-1, -2)
    return R_ij, t_ij

def cam_compose(R_ab, t_ab, R_bc, t_bc):
    R_ac = R_ab @ R_bc
    t_ac = t_bc + (t_ab @ R_bc)
    return R_ac, t_ac

# ----------------------------
# Model: encoder + head + per-camera state
# ----------------------------
class PointEncoder(nn.Module):
    def __init__(self, h=128):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Linear(3, h), nn.ReLU(),
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, h),
        )
        self.post = nn.Sequential(
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, h),
        )
    def forward(self, X):
        z = self.mlp(X)          # (B,P,h)
        z = z.mean(dim=1)        # (B,h)
        z = self.post(z)
        return z

class SE3Head(nn.Module):
    def __init__(self, h=128, e_dim=32):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(h + e_dim, h), nn.ReLU(),
            nn.Linear(h, h), nn.ReLU(),
            nn.Linear(h, 9),  # 6 rot, 3 trans
        )
    def forward(self, feat, e):
        out = self.net(torch.cat([feat, e], dim=-1))
        x6 = out[..., :6]
        t  = out[..., 6:9]
        R  = ortho6d_to_R(x6)
        return R, t

class EcologySE3(nn.Module):
    def __init__(self, h=128, e_dim=32):
        super().__init__()
        self.enc  = PointEncoder(h=h)
        self.head = SE3Head(h=h, e_dim=e_dim)
    def infer_pose(self, X, e):
        feat = self.enc(X)
        return self.head(feat, e)

# ----------------------------
# Population splits (cameras)
# ----------------------------
N_total = 220
N_train = 80
N_test  = 80
N_hold  = N_total - N_train - N_test

train_ids = torch.arange(0, N_train, device=device)
test_ids  = torch.arange(N_train, N_train + N_test, device=device)
hold_ids  = torch.arange(N_train + N_test, N_total, device=device)

e_dim = 32
H = 128

# hidden true SE(3) camera poses (EVAL ONLY)
q = rand_quat(N_total, device=device)
R_true = quat_to_R(q)
t_true = 0.6 * torch.randn(N_total, 3, device=device)

# per-camera persistent state
E = nn.Embedding(N_total, e_dim).to(device)
nn.init.normal_(E.weight, std=0.2)

model = EcologySE3(h=H, e_dim=e_dim).to(device)

# ----------------------------
# Interaction sampler
# ----------------------------
@torch.no_grad()
def sample_pairs(ids, B):
    idx = ids[torch.randint(0, len(ids), (B,), device=device)]
    jdx = ids[torch.randint(0, len(ids), (B,), device=device)]
    mask = idx == jdx
    if mask.any():
        jdx[mask] = ids[torch.randint(0, len(ids), (int(mask.sum().item()),), device=device)]
    return idx, jdx

def interaction_batch(idx, jdx, P=256, noise=0.01):
    B = idx.shape[0]
    S  = sample_scene_from_mesh(B, P)
    Ri = R_true[idx]
    Rj = R_true[jdx]
    ti = t_true[idx]
    tj = t_true[jdx]
    Xi = observe_se3(S, Ri, ti, noise=noise)
    Xj = observe_se3(S, Rj, tj, noise=noise)
    return Xi, Xj

def loss_interaction(Xi, Xj, ei, ej):
    Ri_hat, ti_hat = model.infer_pose(Xi, ei)
    Rj_hat, tj_hat = model.infer_pose(Xj, ej)
    Si_hat = Xi @ Ri_hat + ti_hat[:, None, :]
    Sj_hat = Xj @ Rj_hat + tj_hat[:, None, :]
    loss = F.mse_loss(Si_hat, Sj_hat)
    return loss, (Ri_hat, ti_hat), (Rj_hat, tj_hat)

# ----------------------------
# Phase 1: PRETRAIN (train cameras)
# ----------------------------
def pretrain(steps=4000, B=256, lr=2e-3, P=256, noise=0.01, print_every=500):
    opt = torch.optim.Adam(list(model.parameters()) + list(E.parameters()), lr=lr)
    ema = None
    t0 = time.time()
    for step in range(1, steps+1):
        idx, jdx = sample_pairs(train_ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        loss, _, _ = loss_interaction(Xi, Xj, E(idx), E(jdx))
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()
        l = float(loss.detach())
        ema = l if ema is None else 0.98*ema + 0.02*l
        if step % print_every == 0 or step == 1:
            print(f"pretrain step {step:5d} | loss={l:.6f} | ema={ema:.6f}")
    print(f"pretrain time: {time.time()-t0:.1f}s")

# ----------------------------
# Phase 2: ECOLOGY (state-only updates on test)
# ----------------------------
def bootstrap_ecology(steps=6000, B=256, lr_state=1e-2, P=256, noise=0.01, print_every=500):
    for p in model.parameters():
        p.requires_grad_(False)
    model.eval()
    opt = torch.optim.Adam([E.weight], lr=lr_state)
    ema = None
    t0 = time.time()
    for step in range(1, steps+1):
        idx, jdx = sample_pairs(test_ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        loss, _, _ = loss_interaction(Xi, Xj, E(idx), E(jdx))
        opt.zero_grad(set_to_none=True)
        loss.backward()
        with torch.no_grad():
            g = E.weight.grad
            if g is not None:
                mask = torch.zeros(N_total, device=device, dtype=torch.bool)
                mask[test_ids] = True
                g[~mask] = 0.0
        opt.step()
        l = float(loss.detach())
        ema = l if ema is None else 0.98*ema + 0.02*l
        if step % print_every == 0 or step == 1:
            print(f"eco step {step:5d} | loss={l:.6f} | ema={ema:.6f}")
    print(f"ecology time: {time.time()-t0:.1f}s")

# ----------------------------
# Evaluation: pair MSE
# ----------------------------
@torch.no_grad()
def eval_pair_mse(ids, trials=200, B=256, P=256, noise=0.01):
    model.eval()
    mse = 0.0
    for _ in range(trials):
        idx, jdx = sample_pairs(ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        loss, _, _ = loss_interaction(Xi, Xj, E(idx), E(jdx))
        mse += float(loss)
    return mse / trials

# ----------------------------
# Evaluation: pose distance correlations (hat vs true) on TEST
# ----------------------------
@torch.no_grad()
def eval_pose_corr(ids, B=512, P=256, noise=0.01):
    n = len(ids)
    idx = ids[torch.randint(0, n, (B,), device=device)]
    jdx = ids[torch.randint(0, n, (B,), device=device)]
    mask = idx == jdx
    if mask.any():
        jdx[mask] = ids[torch.randint(0, n, (int(mask.sum().item()),), device=device)]
    Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
    Ri_hat, ti_hat = model.infer_pose(Xi, E(idx))
    Rj_hat, tj_hat = model.infer_pose(Xj, E(jdx))

    # rotation distance corr
    d_hat_R = so3_geodesic(Ri_hat, Rj_hat)
    d_true_R = so3_geodesic(R_true[idx], R_true[jdx])
    d_hat_R = d_hat_R - d_hat_R.mean()
    d_true_R = d_true_R - d_true_R.mean()
    corr_R = (d_hat_R * d_true_R).mean() / (d_hat_R.std() * d_true_R.std() + 1e-9)

    # translation distance corr
    d_hat_t = (ti_hat - tj_hat).norm(dim=-1)
    d_true_t = (t_true[idx] - t_true[jdx]).norm(dim=-1)
    d_hat_t = d_hat_t - d_hat_t.mean()
    d_true_t = d_true_t - d_true_t.mean()
    corr_t = (d_hat_t * d_true_t).mean() / (d_hat_t.std() * d_true_t.std() + 1e-9)

    return float(corr_R), float(corr_t)

# ----------------------------
# Loop closure (LIVE): closure error + relative motion scale
# ----------------------------
@torch.no_grad()
def eval_loop_closure_live(ids, triples=2048, P=256, noise=0.01):
    """
    Returns:
      rot_err_mean_rad, trans_err_mean, rel_trans_scale_mean, rel_rot_scale_mean_rad
    """
    model.eval()
    n = len(ids)
    i = ids[torch.randint(0, n, (triples,), device=device)]
    j = ids[torch.randint(0, n, (triples,), device=device)]
    k = ids[torch.randint(0, n, (triples,), device=device)]
    j = torch.where(j==i, ids[torch.randint(0,n,(triples,),device=device)], j)
    k = torch.where(k==i, ids[torch.randint(0,n,(triples,),device=device)], k)
    k = torch.where(k==j, ids[torch.randint(0,n,(triples,),device=device)], k)

    S = sample_scene_from_mesh(triples, P)
    Xi = observe_se3(S, R_true[i], t_true[i], noise=noise)
    Xj = observe_se3(S, R_true[j], t_true[j], noise=noise)
    Xk = observe_se3(S, R_true[k], t_true[k], noise=noise)

    Ri, ti = model.infer_pose(Xi, E(i))
    Rj, tj = model.infer_pose(Xj, E(j))
    Rk, tk = model.infer_pose(Xk, E(k))

    R_ij, t_ij = cam_rel(Ri, ti, Rj, tj)
    R_jk, t_jk = cam_rel(Rj, tj, Rk, tk)
    R_ki, t_ki = cam_rel(Rk, tk, Ri, ti)

    # relative motion scale (sanity)
    I = torch.eye(3, device=device).expand_as(R_ij)
    rel_rot_scale = so3_geodesic(R_ij, I).mean()
    rel_trans_scale = t_ij.norm(dim=-1).mean()

    # loop closure error
    R_loop, t_loop = cam_compose(R_ij, t_ij, R_jk, t_jk)
    R_loop, t_loop = cam_compose(R_loop, t_loop, R_ki, t_ki)

    Iloop = torch.eye(3, device=device).expand_as(R_loop)
    rot_err = so3_geodesic(R_loop, Iloop).mean()
    trans_err = t_loop.norm(dim=-1).mean()

    return float(rot_err), float(trans_err), float(rel_trans_scale), float(rel_rot_scale)

# ----------------------------
# META-COMPRESSION: estimate one (R*, t*) per camera by averaging model outputs
# ----------------------------
@torch.no_grad()
def meta_compress_test(ids, samples_per_id=48, P=256, noise=0.01):
    """
    Estimate one (R_est, t_est) per camera by averaging predictions across fresh scenes,
    then project R_est to SO(3). This is the compressed ecology object.
    """
    model.eval()
    R_est = torch.zeros(len(ids), 3, 3, device=device)
    t_est = torch.zeros(len(ids), 3, device=device)

    # build mapping from ids -> row index
    for k, sid in enumerate(ids.tolist()):
        sid_t = torch.tensor([sid], device=device)
        Rs = []
        ts = []
        for _ in range(samples_per_id):
            S = sample_scene_from_mesh(1, P)
            X = observe_se3(S, R_true[sid_t], t_true[sid_t], noise=noise)
            R_hat, t_hat = model.infer_pose(X, E(sid_t))
            Rs.append(R_hat[0])
            ts.append(t_hat[0])
        Rm = torch.stack(Rs, dim=0).mean(dim=0)
        Rm = project_to_SO3(Rm.unsqueeze(0))[0]
        tm = torch.stack(ts, dim=0).mean(dim=0)
        R_est[k] = Rm
        t_est[k] = tm
    return R_est, t_est

# ----------------------------
# Compressed evaluation: pair MSE on fresh interactions using only (R_est,t_est)
# ----------------------------
@torch.no_grad()
def eval_pair_mse_compressed(ids, R_est, t_est, trials=200, B=256, P=256, noise=0.01):
    # ids correspond to rows in R_est/t_est
    id_to_row = {int(ids[i].item()): i for i in range(len(ids))}
    mse = 0.0
    for _ in range(trials):
        idx, jdx = sample_pairs(ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)
        ri = torch.tensor([id_to_row[int(x.item())] for x in idx], device=device)
        rj = torch.tensor([id_to_row[int(x.item())] for x in jdx], device=device)
        Si_hat = Xi @ R_est[ri] + t_est[ri][:, None, :]
        Sj_hat = Xj @ R_est[rj] + t_est[rj][:, None, :]
        mse += float(F.mse_loss(Si_hat, Sj_hat))
    return mse / trials

# ----------------------------
# Loop closure (COMPRESSED): closure error + relative motion scale
# ----------------------------
@torch.no_grad()
def eval_loop_closure_compressed(ids, R_est, t_est, triples=4096):
    n = len(ids)
    i = torch.randint(0, n, (triples,), device=device)
    j = torch.randint(0, n, (triples,), device=device)
    k = torch.randint(0, n, (triples,), device=device)
    j = torch.where(j==i, torch.randint(0,n,(triples,),device=device), j)
    k = torch.where(k==i, torch.randint(0,n,(triples,),device=device), k)
    k = torch.where(k==j, torch.randint(0,n,(triples,),device=device), k)

    Ri, ti = R_est[i], t_est[i]
    Rj, tj = R_est[j], t_est[j]
    Rk, tk = R_est[k], t_est[k]

    R_ij, t_ij = cam_rel(Ri, ti, Rj, tj)
    R_jk, t_jk = cam_rel(Rj, tj, Rk, tk)
    R_ki, t_ki = cam_rel(Rk, tk, Ri, ti)

    I = torch.eye(3, device=device).expand_as(R_ij)
    rel_rot_scale = so3_geodesic(R_ij, I).mean()
    rel_trans_scale = t_ij.norm(dim=-1).mean()

    R_loop, t_loop = cam_compose(R_ij, t_ij, R_jk, t_jk)
    R_loop, t_loop = cam_compose(R_loop, t_loop, R_ki, t_ki)

    Iloop = torch.eye(3, device=device).expand_as(R_loop)
    rot_err = so3_geodesic(R_loop, Iloop).mean()
    trans_err = t_loop.norm(dim=-1).mean()

    return float(rot_err), float(trans_err), float(rel_trans_scale), float(rel_rot_scale)

# ----------------------------
# Matched task MSE (same sampled pairs/scenes) live vs compressed
# ----------------------------
@torch.no_grad()
def eval_matched_live_vs_compressed(ids, R_est, t_est, trials=200, B=256, P=256, noise=0.01):
    id_to_row = {int(ids[i].item()): i for i in range(len(ids))}
    live_mse = 0.0
    comp_mse = 0.0
    for _ in range(trials):
        idx, jdx = sample_pairs(ids, B)
        Xi, Xj = interaction_batch(idx, jdx, P=P, noise=noise)

        # LIVE: use model+states
        loss_live, _, _ = loss_interaction(Xi, Xj, E(idx), E(jdx))
        live_mse += float(loss_live)

        # COMP: use extracted (R_est,t_est)
        ri = torch.tensor([id_to_row[int(x.item())] for x in idx], device=device)
        rj = torch.tensor([id_to_row[int(x.item())] for x in jdx], device=device)
        Si_hat = Xi @ R_est[ri] + t_est[ri][:, None, :]
        Sj_hat = Xj @ R_est[rj] + t_est[rj][:, None, :]
        comp_mse += float(F.mse_loss(Si_hat, Sj_hat))

    return live_mse / trials, comp_mse / trials

# ----------------------------
# Reproduce-model error: how close (R_est,t_est) are to model’s outputs
# ----------------------------
@torch.no_grad()
def eval_reproduce_model_error(ids, R_est, t_est, samples=2048, P=256, noise=0.01):
    """
    Sample random camera ids + fresh scenes, compare model's inferred (R_hat,t_hat)
    to compressed (R_est,t_est) for that camera.
    """
    model.eval()
    n = len(ids)
    rows = torch.randint(0, n, (samples,), device=device)
    sid = ids[rows]

    S = sample_scene_from_mesh(samples, P)
    X = observe_se3(S, R_true[sid], t_true[sid], noise=noise)
    R_hat, t_hat = model.infer_pose(X, E(sid))

    R_c = R_est[rows]
    t_c = t_est[rows]

    rot_err = so3_geodesic(R_hat, R_c).mean()
    trans_err = (t_hat - t_c).norm(dim=-1).mean()
    return float(rot_err), float(trans_err)

# ----------------------------
# Byte accounting
# ----------------------------
def bytes_of_params(m):
    return sum(p.numel()*p.element_size() for p in m.parameters())

def bytes_of_embedding_rows(Eweight, ids):
    return int(Eweight[ids].numel() * Eweight.element_size())

# ============================================================
# RUN
# ============================================================

print("\n=== PHASE 1: PRETRAIN (train cameras) ===")
pretrain(steps=4000, B=256, lr=2e-3, P=256, noise=0.01, print_every=500)

print("\n=== EVAL BEFORE ECOLOGY (test cameras) ===")
mse_before = eval_pair_mse(test_ids, trials=200, B=256, P=256, noise=0.01)
corrR_before, corrt_before = eval_pose_corr(test_ids, B=512, P=256, noise=0.01)
loopR_before, loopt_before, relt_before, relR_before = eval_loop_closure_live(test_ids, triples=2048, P=256, noise=0.01)

print(f"pair MSE test->test (before ecology): {mse_before:.6f}")
print(f"corr(rot-dist(hat), rot-dist(true)) test (before): {corrR_before:.4f}")
print(f"corr(trans-dist(hat), trans-dist(true)) test (before): {corrt_before:.4f}")
print(f"loop closure (LIVE) rot-err mean (rad) before: {loopR_before:.6f}")
print(f"loop closure (LIVE) trans-err mean before:     {loopt_before:.3e}")
# (optional but useful sanity scale)
print(f"relative motion (LIVE) rot-scale mean (rad):   {relR_before:.3f}")
print(f"relative motion (LIVE) trans-scale mean:       {relt_before:.3f}")

print("\n=== PHASE 2: ECOLOGY BOOTSTRAP (test cameras; state-only updates) ===")
bootstrap_ecology(steps=6000, B=256, lr_state=1e-2, P=256, noise=0.01, print_every=500)

print("\n=== EVAL AFTER ECOLOGY (test cameras) ===")
mse_after = eval_pair_mse(test_ids, trials=200, B=256, P=256, noise=0.01)
corrR_after, corrt_after = eval_pose_corr(test_ids, B=512, P=256, noise=0.01)
loopR_after, loopt_after, relt_after, relR_after = eval_loop_closure_live(test_ids, triples=2048, P=256, noise=0.01)

print(f"pair MSE test->test (after ecology): {mse_after:.6f}")
print(f"corr(rot-dist(hat), rot-dist(true)) test (after): {corrR_after:.4f}")
print(f"corr(trans-dist(hat), trans-dist(true)) test (after): {corrt_after:.4f}")
print(f"loop closure (LIVE) rot-err mean (rad) after:  {loopR_after:.6f}")
print(f"loop closure (LIVE) trans-err mean after:      {loopt_after:.3e}")
print(f"relative motion (LIVE) rot-scale mean (rad):   {relR_after:.3f}")
print(f"relative motion (LIVE) trans-scale mean:       {relt_after:.3f}")

print("\n=== META-ECOLOGY: extract canonical SE(3) per TEST camera (compressed object) ===")
R_est, t_est = meta_compress_test(test_ids, samples_per_id=48, P=256, noise=0.01)

mse_comp_fresh = eval_pair_mse_compressed(test_ids, R_est, t_est, trials=200, B=256, P=256, noise=0.01)
live_mse_task, comp_mse_task = eval_matched_live_vs_compressed(test_ids, R_est, t_est, trials=200, B=256, P=256, noise=0.01)
ratio_task = comp_mse_task / (live_mse_task + 1e-12)

rot_rep_err, trans_rep_err = eval_reproduce_model_error(test_ids, R_est, t_est, samples=2048, P=256, noise=0.01)
loopR_comp, loopt_comp, relt_comp, relR_comp = eval_loop_closure_compressed(test_ids, R_est, t_est, triples=4096)

print(f"compressed pair MSE test->test (fresh):              {mse_comp_fresh:.6f}")
print(f"matched MSE (task): live={live_mse_task:.6f}  comp={comp_mse_task:.6f}")
print(f"losslessness (matched task ratio comp/live):        {ratio_task:.4f}   (can be <1 due to denoising)")
print(f"reproduce-model error: mean rot angle (rad):        {rot_rep_err:.6f}")
print(f"reproduce-model error: mean trans L2:               {trans_rep_err:.6f}")
print(f"loop closure (COMPRESSED) rot-err mean (rad):       {loopR_comp:.6f}")
print(f"loop closure (COMPRESSED) trans-err mean:           {loopt_comp:.3e}")
print(f"relative motion (COMPRESSED) rot-scale mean (rad):  {relR_comp:.3f}")
print(f"relative motion (COMPRESSED) trans-scale mean:      {relt_comp:.3f}")

# ----------------------------
# Compression report
# ----------------------------
model_bytes = bytes_of_params(model)
etest_bytes = bytes_of_embedding_rows(E.weight, test_ids)

# Store compressed as float32: R_est + t_est
compressed_f32_bytes = int(R_est.numel()*4 + t_est.numel()*4)
compressed_f16_bytes = int(R_est.numel()*2 + t_est.numel()*2)  # estimate if stored as float16

orig_total = model_bytes + etest_bytes

print("\n=== COMPRESSION REPORT (TEST cameras) ===")
print(f"Original (model params):              {model_bytes/1024:.1f}KB")
print(f"Original (TEST states E[test]):       {etest_bytes/1024:.1f}KB")
print(f"Original total (model + E[test]):     {orig_total/1024:.1f}KB")
print(f"Compressed (R_est+t_est float32):     {compressed_f32_bytes/1024:.1f}KB")
print(f"Compressed (R_est+t_est float16 est): {compressed_f16_bytes/1024:.1f}KB")
print(f"Compression ratio (orig / f32):       {orig_total/compressed_f32_bytes:.1f}x")
print(f"Compression ratio (orig / f16):       {orig_total/compressed_f16_bytes:.1f}x")

print("\nDone.")

"""
---

# Non-Abelian Delta-Ecology for SE(3) Camera Networks

## Exposure-Only State Bootstrapping and Meta-Compression

### Abstract

We demonstrate a simple “delta-ecology” procedure on a synthetic **SE(3)** camera network where **unseen cameras** (test identities) acquire stable poses using **exposure only**, i.e. by updating only persistent per-camera state embeddings while keeping the perceptual model frozen. The training signal is purely **relational**: two cameras co-observe the same scene and must agree on a recovered canonical scene, with no camera labels or ground-truth poses used in the ecology phase. After exposure, inferred poses for test cameras become geometrically meaningful (high correlation with true relative distances). We then show a **meta-ecology compression**: the stabilized test-camera states can be distilled into one canonical SE(3) element per camera ((\hat R_i,\hat t_i)), yielding a (\sim 10^2)–(10^3) reduction in storage while preserving task performance up to small error; in our runs the compressed representation can even appear “better” under the task MSE due to denoising via averaging and projection.

---

## 1. Setup

### 1.1 Group and camera model

Each camera (i) has a latent pose (g_i \in \mathrm{SE}(3)) with rotation (R_i\in\mathrm{SO}(3)) and translation (t_i\in\mathbb{R}^3). We use a standard world→camera observation model on point clouds:

[
X_i = (S - t_i),R_i^\top + \epsilon,
]

where (S\in\mathbb{R}^{P\times 3}) is a 3D scene (point set), (X_i) are the camera-frame points, and (\epsilon) is noise.

A canonical “scene reconstruction” from an estimated pose ((\hat R_i,\hat t_i)) is

[
\hat S_i = X_i \hat R_i + \hat t_i.
]

### 1.2 Interaction primitive (the ecology edge)

An **interaction** is a pair of cameras ((i,j)) that co-observe the same scene (S). The only supervision is that their recovered canonical scenes should match:

[
\mathcal{L}_{\text{pair}}(i,j; S)
= \big|,\hat S_i - \hat S_j,\big|_2^2
= \big|,X_i\hat R_i+\hat t_i ;-; (X_j\hat R_j+\hat t_j),\big|_2^2.
]

No absolute coordinate system is given; the loss is invariant to a **global gauge transform** applied consistently to all cameras.

### 1.3 Model: perception + persistent identity state

Each camera (i) also owns a persistent state vector (e_i \in \mathbb{R}^{d}) (an embedding). A neural model predicts pose from an observation plus the camera state:

[
(\hat R_i,\hat t_i) = f_\theta(X_i, e_i).
]

Implementation detail: the model factorizes into a point encoder (g_\theta(X)) and a head (h_\theta(\cdot)) that outputs a 6D rotation representation (mapped to (\mathrm{SO}(3)) via orthonormalization) plus translation.

---

## 2. Two-phase protocol

We split cameras into **train** and **test** identities.

### Phase 1: Pretrain (train cameras)

We optimize both network parameters (\theta) and all embeddings (e_i) (including train cameras) on random interactions among train cameras:

[
\min_{\theta,{e_i}}
;\mathbb{E}*{(i,j)\sim \text{train pairs},,S}
\big[\mathcal{L}*{\text{pair}}(i,j;S)\big].
]

Goal: learn a generic pose-inference mechanism that can be “steered” by a camera identity state.

### Phase 2: Ecology bootstrap (test cameras, exposure-only)

We **freeze** (\theta) and update **only** test embeddings ({e_i : i\in\text{test}}) using interactions among test cameras:

[
\min_{{e_i}*{i\in\text{test}}}
;\mathbb{E}*{(i,j)\sim \text{test pairs},,S}
\big[\mathcal{L}_{\text{pair}}(i,j;S)\big],
\quad \theta;\text{fixed}.
]

This is the core claim tested: **unseen systems self-organize their internal states from relational constraints alone**.

---

## 3. Evaluation metrics

### 3.1 Task metric: pairwise scene consistency

We report mean pair MSE on fresh interactions:

[
\text{MSE}*{\text{pair}} = \mathbb{E}\big[\mathcal{L}*{\text{pair}}\big].
]

### 3.2 Geometry alignment: relative distance correlations

Using ground-truth poses only for evaluation, we compare relative distances:

* Rotation distance via SO(3) geodesic:
  [
  d_R(i,j) = \arccos!\left(\frac{\mathrm{tr}(R_i^\top R_j)-1}{2}\right).
  ]

* Translation distance:
  [
  d_t(i,j) = |t_i - t_j|_2.
  ]

We compute correlations between predicted distances (\hat d_R(i,j)), (\hat d_t(i,j)) (from (\hat R,\hat t)) and the true (d_R,d_t).

### 3.3 Loop closure diagnostics (internal consistency)

We also measure “loop closure” style consistency (implementation-dependent but conceptually):

* **Rotation loop error**: mean angle of the residual rotation after composing predicted relative transforms around a loop.
* **Translation loop error**: mean norm of residual translation after loop composition.

These aim to distinguish “internally self-consistent but mis-gauged” from “globally aligned.”

### 3.4 Meta-ecology compression quality

We produce a compressed representation by estimating a single canonical pose per camera by averaging predicted poses over many fresh scenes and projecting to (\mathrm{SO}(3)) (SVD projection for rotation):

[
\bar R_i \leftarrow \mathrm{Proj}*{\mathrm{SO}(3)}\Big(\frac{1}{K}\sum*{k=1}^K \hat R_i^{(k)}\Big),
\quad
\bar t_i \leftarrow \frac{1}{K}\sum_{k=1}^K \hat t_i^{(k)}.
]

Then we evaluate the same pairwise task but using only ((\bar R_i,\bar t_i)). We report:

* (\text{MSE}_{\text{pair}}^{\text{comp}}) on fresh interactions using ((\bar R,\bar t)).
* A “matched task ratio”:
  [
  \rho = \frac{\text{MSE}*{\text{pair}}^{\text{comp}}}{\text{MSE}*{\text{pair}}^{\text{live}}}.
  ]
  Note: (\rho < 1) can happen if compression denoises.

We also report **reproduce-model error** between live predicted poses and compressed poses (mean rotation angle + translation L2), i.e. how well the compressed object replicates the stabilized model’s behavior.

---

## 4. Results (from the provided run)

### 4.1 Phase 1: pretrain

Training pair loss rapidly collapses from ~2.89 to (\approx 3\times 10^{-4}) on train interactions, indicating the model can solve the relational reconstruction task when camera identities are within the training set.

### 4.2 Before ecology (test cameras)

* Pair MSE (test→test): **2.551592**
* Corr(rot-dist(hat), rot-dist(true)): **0.0167**
* Corr(trans-dist(hat), trans-dist(true)): **0.4607**
* Loop closure (LIVE) rot-err mean: **2.21e-4 rad**
* Loop closure (LIVE) trans-err mean: **1.52e-7**
* Relative motion scale (LIVE): rot ~ **2.088 rad**, trans ~ **0.819**

Interpretation: the frozen model can be internally consistent (tiny loop error) while still failing to align test identities to meaningful geometry (near-zero rotation distance correlation). This is consistent with a “mis-gauged” state where the system is coherent but not yet anchored to the correct identity assignments.

### 4.3 After ecology (test cameras, state-only updates)

* Pair MSE (test→test): **0.000298**
* Corr(rot-dist): **0.9980**
* Corr(trans-dist): **0.9980**
* Loop closure (LIVE) rot-err: **2.14e-4 rad**
* Loop closure (LIVE) trans-err: **2.66e-7**
* Relative motion scale (LIVE): rot ~ **2.216 rad**, trans ~ **1.459**

Interpretation: updating only (e_i) for test cameras is sufficient to “gauge-fix” the unseen cameras—poses become geometrically meaningful and the task error collapses by ~four orders of magnitude.

### 4.4 Meta-ecology compression (canonical SE(3) per test camera)

Using only one ((R_{\text{est}},t_{\text{est}})) per test camera:

* Compressed pair MSE (fresh): **0.000277**
* Matched task MSE: live **0.000300** vs comp **0.000277**
* Matched task ratio: **0.9214** (can be <1 due to denoising)
* Reproduce-model error: mean rot angle **0.002433 rad**, mean trans L2 **0.002521**
* Loop closure (COMP): rot-err **0.000573**, trans-err **1.20e-6**
* Relative motion scale (COMP): rot ~ **2.190 rad**, trans ~ **1.457**

Interpretation: the compressed representation preserves the stabilized ecology’s behavior extremely well, to within ~(2.4\times 10^{-3}) rad (~0.14°) and ~(2.5\times 10^{-3}) translation units, while matching or slightly improving the task MSE (consistent with averaging/projection acting as denoising).

### 4.5 Storage and compression ratios

Reported:

* Original (model params): **409.5 KB**
* Original (test states (E[\text{test}])): **10.0 KB**
* Original total: **419.5 KB**
* Compressed (R+t float32): **3.8 KB**
* Compressed (R+t float16 est): **1.9 KB**
* Compression ratio: **111.9× (f32)**, **223.8× (f16)**

Important nuance: this compression is of the **ecology’s stabilized solution object** (per-camera group elements), not of raw point cloud data. It is “lossless” only with respect to the **task behavior** being measured (scene-consistency under this generator), and “approximately lossless” with respect to reproducing the model’s live outputs.

---

## 5. What this experiment shows (and what it does not)

### Demonstrated by the run

1. **Exposure-only bootstrapping for unseen identities**: with (\theta) frozen, persistent states (e_i) for test cameras can be learned purely from relational interaction losses, producing high-fidelity SE(3) structure alignment (corr ~ 0.998).
2. **Meta-compression**: once stabilized, the high-dimensional identity states can be distilled into a small canonical group element per camera with large storage reduction and minimal degradation under the measured task (and sometimes apparent improvement due to denoising).

### Not demonstrated (yet)

1. **General data compression** in the sense of coding arbitrary signals to fewer bits than standard codecs. Here, we compress a *learned relational solution* (camera identities in SE(3)), not arbitrary geometry or images.
2. **Information-theoretic optimality**: no rate–distortion curve, no entropy model, no bitstream; the “KB” accounting is parameter storage, not an actual codec.
3. **Uniqueness / global identifiability**: the underlying objective is gauge-invariant; the ecology picks a consistent gauge and identity assignment, but formal identifiability conditions are not established in this write-up.

---

## 6. Why non-abelian structure matters (hypothesis)

In SE(3), composition order matters, and loop constraints carry nontrivial structure. The ecology objective repeatedly enforces pairwise agreement across many overlapping interactions, effectively propagating and stabilizing identity-specific “frames.” A plausible mechanism is:

* Pairwise losses produce **deltas** (mis-alignments) between cameras.
* Persistent states provide memory to accumulate those deltas across exposures.
* Non-commutativity prevents contradictions from simply averaging away; instead, consistency across many loops forces a coherent assignment (a gauge fixing).

This is a conceptual interpretation; the experiment provides strong empirical evidence for the phenomenon in this synthetic setting, but does not yet constitute a general theorem.

---

## 7. Reproducibility notes

* Synthetic generator: fixed underlying set of mesh vertices sampled per scene; per-scene normalization; Gaussian noise.
* Model: PointNet-style mean pooling encoder + head producing ((R,t)) conditioned on embedding.
* Phase 2 masks gradients so only test embedding rows update.
* Meta-compression: average predictions across scenes; project rotation to SO(3) via SVD.

---
"""

